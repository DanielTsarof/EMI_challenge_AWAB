{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692a09d3-da7b-4fd2-908b-fad2b6152e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c34c72-666f-4cb7-bf65-a2bdc0eb35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/media/dtsarev/SatSSD/data/face_images'\n",
    "TRAIN_CSV = os.path.join('/media/dtsarev/SatSSD/data', 'train_split.csv')\n",
    "VAL_CSV   = os.path.join('/media/dtsarev/SatSSD/data', 'valid_split.csv')\n",
    "EMOTIONS = ['Admiration', 'Amusement', 'Determination', 'Empathic Pain', 'Excitement', 'Joy']\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 1  # One directory per batch\n",
    "CHUNK_SIZE = 16  # Process this many images at a time to limit memory\n",
    "LR = 1e-4\n",
    "EPOCHS = 10\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e874809-33f1-4f45-86d1-08a6f66ae75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(preds, targets):\n",
    "    \"\"\"\n",
    "    Compute mean Pearson correlation across all emotions.\n",
    "    \"\"\"\n",
    "    preds = preds.detach().cpu()\n",
    "    targets = targets.detach().cpu()\n",
    "    vx = preds - preds.mean(0)\n",
    "    vy = targets - targets.mean(0)\n",
    "    corr = (vx * vy).sum(0) / (torch.sqrt((vx**2).sum(0) * (vy**2).sum(0)) + 1e-8)\n",
    "    return corr.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08dfc2fc-c1a6-43b8-9ab9-32e4dea0391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceEmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads all images from each subject directory and corresponding emotion labels.\n",
    "    Filters out directories with no images.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, data_dir, transform=None):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.emotions = EMOTIONS\n",
    "        self.samples = []  # list of (dir_path, label_tensor, image_files)\n",
    "        for _, row in df.iterrows():\n",
    "            dir_name = f\"{int(row['Filename']):05d}\"\n",
    "            dir_path = os.path.join(self.data_dir, dir_name)\n",
    "            if not os.path.isdir(dir_path):\n",
    "                continue\n",
    "            files = [f for f in os.listdir(dir_path) if f.lower().endswith('.jpg')]\n",
    "            if not files:\n",
    "                continue\n",
    "            labels = torch.tensor([row[e] for e in self.emotions], dtype=torch.float32)\n",
    "            self.samples.append((dir_path, labels, files))\n",
    "        if len(self.samples) < len(df):\n",
    "            print(f\"Warning: {len(df) - len(self.samples)} samples removed due to missing images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dir_path, labels, files = self.samples[idx]\n",
    "        images = []\n",
    "        for fname in files:\n",
    "            file_path = os.path.join(dir_path, fname)\n",
    "            # Open and close file handle properly to avoid too many open files\n",
    "            with Image.open(file_path) as img_src:\n",
    "                img = img_src.convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48acbd85-9203-421e-a80a-ff90522a31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images_list = [item[0] for item in batch]  # list of list of img tensors\n",
    "    labels = torch.stack([item[1] for item in batch], dim=0)\n",
    "    return images_list, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892123b9-ef98-4a87-a305-111aab853da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 119 samples removed due to missing images.\n",
      "Warning: 23 samples removed due to missing images.\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = FaceEmotionDataset(TRAIN_CSV, DATA_DIR, transform=train_transform)\n",
    "val_dataset   = FaceEmotionDataset(VAL_CSV, DATA_DIR, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236ed05c-c038-47ac-9ab8-4a6bba7e914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet50', pretrained=True, num_classes=len(EMOTIONS))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "824b3b00-9e43-45b9-9a94-55b92c7d14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1f2c7b3-9ed8-4424-97d4-188251f2200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_predictions(model, images, device):\n",
    "    \"\"\"\n",
    "    Predict on a list of image tensors in chunks, sum then average.\n",
    "    \"\"\"\n",
    "    total = torch.zeros(len(EMOTIONS), device=device)\n",
    "    count = 0\n",
    "    for i in range(0, len(images), CHUNK_SIZE):\n",
    "        chunk = images[i:i+CHUNK_SIZE]\n",
    "        tensor = torch.stack(chunk, dim=0).to(device)\n",
    "        with torch.set_grad_enabled(model.training):\n",
    "            preds = model(tensor)  # shape [chunk_size, num_emotions]\n",
    "        total += preds.sum(dim=0)\n",
    "        count += preds.size(0)\n",
    "        # free memory\n",
    "        del tensor, preds\n",
    "        torch.cuda.empty_cache()\n",
    "    avg = total / count\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb647b2c-3068-4688-ae39-33530e91e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for images_list, labels in tqdm(loader):\n",
    "        # batch size is 1: one directory per batch\n",
    "        images = images_list[0]\n",
    "        labels_dir = labels.to(device)\n",
    "        dir_size = len(images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # chunk-wise forward and backward to limit memory\n",
    "        total_loss = 0.0\n",
    "        for i in range(0, dir_size, CHUNK_SIZE):\n",
    "            chunk = images[i:i+CHUNK_SIZE]\n",
    "            tensor = torch.stack(chunk, dim=0).to(device)\n",
    "            outputs = model(tensor)  # [chunk_size, num_emotions]\n",
    "            # repeat label for each frame\n",
    "            labels_rep = labels_dir.repeat(outputs.size(0), 1)\n",
    "            loss_chunk = criterion(outputs, labels_rep)\n",
    "            # weight by fraction of frames\n",
    "            weight = outputs.size(0) / dir_size\n",
    "            (loss_chunk * weight).backward()\n",
    "            total_loss += loss_chunk.item() * weight\n",
    "            # free memory\n",
    "            del tensor, outputs, labels_rep\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += total_loss\n",
    "\n",
    "        # compute prediction for metrics (no grad)\n",
    "        with torch.no_grad():\n",
    "            avg_pred = aggregate_predictions(model, images, device)\n",
    "        all_preds.append(avg_pred.detach().cpu())\n",
    "        all_targets.append(labels_dir.detach().cpu())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    preds_cat = torch.stack(all_preds)\n",
    "    targets_cat = torch.stack(all_targets)\n",
    "    epoch_corr = pearson_corr(preds_cat, targets_cat)\n",
    "    return epoch_loss, epoch_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58adf9c3-4137-4818-a46d-58d9c3a47a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for images_list, labels in tqdm(loader):\n",
    "            batch_preds = []\n",
    "            for images in images_list:\n",
    "                avg_pred = aggregate_predictions(model, images, device)\n",
    "                batch_preds.append(avg_pred)\n",
    "            preds = torch.stack(batch_preds, dim=0)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            loss = criterion(preds, labels)\n",
    "            running_loss += loss.item() * preds.size(0)\n",
    "            all_preds.append(preds.detach().cpu())\n",
    "            all_targets.append(labels.detach().cpu())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    preds_cat = torch.cat(all_preds)\n",
    "    targets_cat = torch.cat(all_targets)\n",
    "    overall_corr = pearson_corr(preds_cat, targets_cat)\n",
    "    per_emotion_corrs = []\n",
    "    for i in range(targets_cat.size(1)):\n",
    "        vx = preds_cat[:, i] - preds_cat[:, i].mean()\n",
    "        vy = targets_cat[:, i] - targets_cat[:, i].mean()\n",
    "        corr = (vx * vy).sum() / (torch.sqrt((vx**2).sum() * (vy**2).sum()) + 1e-8)\n",
    "        per_emotion_corrs.append(corr.item())\n",
    "    return epoch_loss, overall_corr, per_emotion_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5506a0a8-6f7d-4440-9e92-a73f77d15bfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7953/7953 [41:33<00:00,  3.19it/s]\n",
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 3604/4565 [03:29<00:55, 17.38it/s]Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dtsarev/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3027, in write\n",
      "    result = original_write(data, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/dtsarev/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/ipykernel/iostream.py\", line 692, in write\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/home/dtsarev/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/ipykernel/iostream.py\", line 269, in schedule\n",
      "    f()\n",
      "  File \"/home/dtsarev/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/ipykernel/iostream.py\", line 649, in _flush\n",
      "    self.session.send(\n",
      "  File \"/home/dtsarev/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/jupyter_client/session.py\", line 863, in send\n",
      "    stream.send_multipart(to_send, copy=copy)\n",
      "  File \"/home/dtsarev/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/ipykernel/iostream.py\", line 276, in send_multipart\n",
      "    self.schedule(lambda: self._really_send(*args, **kwargs))\n",
      "  File \"/home/dtsarev/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/ipykernel/iostream.py\", line 269, in schedule\n",
      "    f()\n",
      "  File \"/home/dtsarev/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/ipykernel/iostream.py\", line 276, in <lambda>\n",
      "    self.schedule(lambda: self._really_send(*args, **kwargs))\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/dtsarev/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/ipykernel/iostream.py\", line 292, in _really_send\n",
      "    ctx, pipe_out = self._setup_pipe_out()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/dtsarev/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/ipykernel/iostream.py\", line 207, in _setup_pipe_out\n",
      "    ctx = zmq.Context()\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/dtsarev/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/zmq/sugar/context.py\", line 114, in __init__\n",
      "    super().__init__(io_threads=io_threads, shadow=shadow_address)\n",
      "  File \"_zmq.py\", line 552, in zmq.backend.cython._zmq.Context.__init__\n",
      "zmq.error.ZMQError: Too many open files\n",
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                 | 3604/4565 [2:08:19<34:12,  2.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m      7\u001b[39m     train_loss, train_corr = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     val_loss, val_corr, val_corr_per_emotion = \u001b[43meval_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     scheduler.step()\n\u001b[32m     11\u001b[39m     train_losses.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36meval_epoch\u001b[39m\u001b[34m(model, loader, criterion, device)\u001b[39m\n\u001b[32m      5\u001b[39m all_targets = []\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_preds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages_list\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master_of_cv/DIPLOM/repos/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.11/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.11/multiprocessing/connection.py:256\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.11/multiprocessing/connection.py:423\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.11/multiprocessing/connection.py:930\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    927\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m    932\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.11/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28mself\u001b[39m._selector.poll(timeout)\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_losses, train_corrs = [], []\n",
    "val_losses, val_corrs = [], []\n",
    "best_val_corr = -1\n",
    "best_model_wts = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_corr = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_corr, val_corr_per_emotion = eval_epoch(model, val_loader, criterion, DEVICE)\n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_corrs.append(train_corr)\n",
    "    val_losses.append(val_loss)\n",
    "    val_corrs.append(val_corr)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f}, Train Corr: {train_corr:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Corr: {val_corr:.4f}\")\n",
    "    if val_corr > best_val_corr:\n",
    "        best_val_corr = val_corr\n",
    "        best_model_wts = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814db532-70bf-47e5-a94e-2bacc422aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad83ce5-2470-4b33-bbbb-53a2c52d75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1, EPOCHS+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, EPOCHS+1), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(1, EPOCHS+1), train_corrs, label='Train Pearson')\n",
    "plt.plot(range(1, EPOCHS+1), val_corrs, label='Val Pearson')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Pearson Correlation')\n",
    "plt.title('Correlation over Epochs')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3be1a0-b832-430f-9132-c1af0da09f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Per-emotion Pearson correlation on validation set:')\n",
    "for emo, corr in zip(EMOTIONS, val_corr_per_emotion):\n",
    "    print(f\"{emo}: {corr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
